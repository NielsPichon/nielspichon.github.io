---
layout: post
title:  "LoRa for convolutional networks"
date:   2023-11-06 09:46:03 +0200
categories: AI LoRa ComputerVision
mathjax: true
---


One of the coolest papers I have read as of late, are the [LoRA paper](https://arxiv.org/pdf/2106.09685.pdf) (2021) and its more recent successor on [QLoRA](https://arxiv.org/pdf/2305.14314.pdf) (2023). At the highest level, LoRA is a conceptually simple yet very elegant solution for fine-tuning Large Language Models while using significantly less GPU VRAM. And as been widely adopted as a good solution for fine-tuning large models with less resources. Now, I have mostly been dealing with convolution networks as of late, playing with medical images. The original paper focused on transformers and by extension, linear layers. But it turns out that the concept can easily be extended to convolutional layers too! And this is what we are going to explore in this article.

## The gist of LoRA

As a starting point, we need to acknowledge that large language models, and large deep learning models in general are over parametrized. As such, a significant portion of the weights are close to zero, and thus, if we look at the weight matrix of a linear layer (such as those in the transformers), their rank is much lower than their dimension. So in theory we could compress the information in this layer by a large factor without loosing any information. This translates into a matrix multiplication:

$$W_{nm} = A_{nr} * B_{rm}$$

where $r$ is the rank of the weights matrix, $W$ is the original weight matrix (taking n input features and m output features), and A and B are 2 lower rank matrices.

Of course we don't really know the true rank of the weights so we will need to play around with the low rank as an hyperparameter. Now, upon fine-tuning a model for a specific task, we can freeze the original weights, and, add 2 linear layers with no bias in parallel to each linear layer:

$$W'_{nm} = W_{nm} + A_{nr} * B_{rm}$$

This is interesting because with the correct choice of $r$ we get a large reduction in weight number. For instance if $r$ is 10% of $n$, in a self-attention where $n = m$, we get an 80% reduction in weights, and thus in gradient and momenta buckets memory usage too (the features' memory cost scales with $O(n)$ here while the others scale with $O(n^2)$, so they don't really matter).

Now when inference time comes, we can simply compute the updated $W'$ weights, and we will retain the original network's compute cost, removing the extra training layers we created.

Last but not least, in the context where we may have a lot of different models with the same backbone, and we need to switch models on the fly a lot at inference time, one thing that can be done is to keep the pretrained model in memory. Then we can simply transfer to the GPU the weights of $A$ and $B$, which again is much cheaper, and apply them in the GPU. When we want to change weights, we can simply subtract the no longer needed fine-tuned weights and load some new ones. This does save a lot of I/O, which for large model can become a bottleneck, especially in the context of limited hardware resources.

## Extending it to convolutional layers

It turns out the decomposition above is nothing more than an approximation of the [CP decomposition](https://en.wikipedia.org/wiki/Tensor_rank_decomposition) of the weight tensor, and in a more general context, the weight tensor. As such we can apply it to pretty much any weight tensor, such as those of convolutional layers, which are 2 + n-D (output channels, input output, spatial dims). The only difference is that we need to apply the CP decomposition to the weight tensor, and not the weight matrix. Following [this paper](https://arxiv.org/abs/1412.6553), what we have, for a 2D convolutional layer is this:

$$Y(t, x, y) = \Sigma_s \Sigma_i \Sigma_j K(t, s, x + i, y + j) * X(s, x + i, y + j)$$

where $Y$ is the output, $X$ the input, $K$ the kernel, and $r$ the output channel. The sum over $i$ and $j$ refer to sum over the elements of the kernel, and the sum over $s$ to that over the input channels. Now, we can apply the CP decomposition to the kernel. We use a first 1x1 kernel which projects to fewer channels, followed by a lower rank kernel which mirrors the original one (same padding, stride, kernel shape...), and a final 1x1 kernel which projects back to the desired output channels count:

$$X_{low}(r, x, y) = \Sigma_s K_{low}(r, s) * X(s, x, y)$$

$$X_{mid}(r', x, y) = \Sigma_r \Sigma_i \Sigma_j K_{mid}(r', ', x + i, y + j) * X_{low}(r, x + i, y + j)$$

$$Y(t, x, y) = \Sigma_{r'} K_{up}(t, r') * X_{mid}(r', x, y)$$

Note that $r'$ has the same size as r. It is merely a notation choice for readability. Also, it is worth noting that the compared to the original paper we kept a 2D kernel rather than two 1xk convolutions (a separable 2D kernel that is), following the implementation of [LyCoris](https://github.com/KohakuBlueleaf/LyCORIS/blob/main/lycoris/modules/locon.py#L42).

So basically what we have is 3 convolution layers. However, the LyCoris implementation did not show any way of recomposing the original convolution weights from these. So what we do is combine all 3 equations above:

$$Y(t, x, y) = \Sigma_{r'} K_{up}(t, r') * (\Sigma_r \Sigma_i \Sigma_j K_{mid}(r', r, x + i, y + j) * (\Sigma_s K_{low}(r, s) * X(s, x + i, y + j)))$$

Rearranging the sums, we get:

$$Y(t, x, y) = \Sigma_s \Sigma_i \Sigma_j \left(\Sigma_{r'} K_{up}(t, r') * \Sigma_r  K_{mid}(r', r, x + i, y + j) * K_{low}(r, s)\right) * X(s, x + i, y + j)$$

It follows that we have:

$$K(t, s, x + i, y + j) = \Sigma_{r'} K_{up}(t, r') * \Sigma_r  K_{mid}(r', r, x + i, y + j) * K_{low}(r, s)$$

Using pytorch we can translate this as follows:

```python
kernel = torch.zeros_like(original_kernel)
tmp = torch.zeros(low_rank, *original_kernel.shape[1:])
for rr in range(low_rank):
    for s in range(in_channels):
        tmp[rr, s] = torch.sum(mid_kernel[rr, :] * low_kernel[:, s].transpose(0, 1).reshape(in_channels, -1),
                               dim=1)

for t in range(out_channels):
    for s in range(in_channels):
        kernel[t,s] = torch.sum(tmp[:, s] * up_kernel[t, :].transpose(0, 1).reshape(in_channels, -1),
                                dim=1)
```

This is somewhat ugly because of the 2 for loops, but because the loops are over the channel dimensions, this is actually very few iterations in most cases. There is probably room for more vectorization here, but I'll leave that for another day.

Now, the CP decomposition is not the only way we could have approached this problem. For instance, the LoRA implementation of [HuggingFace's diffusers](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/lora.py#L77) uses a single down convolution with the original kernel's shape, followed by a 1x1 convolution which projects to the desired output channels. The LyCoris repo also offers this option.

Now if we count the number of parameters, what we get is $r^2 \times k^2 + s \times r + t \times r$ for the full CP decomposition VS $s \times r \times k^2 + r \times t$ for HuggingFace's version (assuming we have a kxk kernel originally). This means that whenever $r < s \times \frac{k^2 - 1}{k^2}$ the CP decomposition will save more parameters.

We can further compute the max low rank for which this decomposition is useful as compared to using a straight up conv layer:

$$ r^2 \times k^2 + s \times r + t \times r < s \times t \times k^2 $$

Solving for $r$ we get:

$$ r < \frac{-\frac{t + s}{k^2} + \sqrt{\frac{(t + s)^2}{k^4} + 4 \times s \times t}}{2}$$

We can use a stricter bound:

$$ r < -max(t, s)\frac{1}{k^2} + min(t, s)\sqrt{\frac{1}{k^4} + 1}$$

In the simple decomposition case we get $r < \frac{t \times s \times k^2}{s \times k^2 + t}$, which, we can make more strict as follows: $r < \frac{min(t, s) ^2}{max(t, s)}\times \frac{k^2}{k^2 + 1}$.

As an example, in the case where we have $t = s$ and 3x3 kernel, it becomes interesting to use the simple HuggingFace conv LoRA for $r < 0.9 s$, the CP decomposition for $r < 0.89 s$, and more interesting to use the CP decomposition than the simple version for $r < 0.88 s$. So in practice, the CP decomposition is almost always better given we often aim at ranks on the order of 10% of the weight dimensions for large networks.

## Final words

I'll conclude with a cool idea I had. Now that we have this LoRA schema we can push it one step further by fine-tuning a pre-trained 2D network into a 3D one. All we need is to convert the 2D kernel into a 3D one like so:

```math
[[a, b, c],
 [d, e, f],
 [g, h, i]]
```

↓↓↓

```math
[[[0, 0, 0],
  [0, 0, 0],
  [0, 0, 0]],
 [[a, b, c],
  [d, e, f],
  [g, h, i]],
 [[0, 0, 0],
  [0, 0, 0],
  [0, 0, 0]]]
```

The above example is for 3x3 kernel, except it could be applied to any odd 2D kernel shape. With a padding of 1 and a stride of 1 along the new dimension, the network is strictly equivalent to the original one, except it now takes as input an extra height dimension. Then one can add some fine-tuning using the Conv LoRA described above. In the case where there is a linear layer somewhere, one can add a bit of reshaping to merge the height dimension into the batch dimension, run the layer, and then unmerge the height dim. This typically applies to the attention mechanism that can be found in the bottleneck of some larger convolutional networks (e.g. stable diffusion XL VAE). Maybe this could open the door for some 3D volumetric image generation and interesting graph based approaches?
